{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97613e835de9374",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "## Introduction\n",
    "This notebook focuses on cleaning and preprocessing the Airbnb datasets for Paris and Los Angeles. Our goal is to prepare the data for analysis by addressing missing values, removing duplicates, and ensuring consistency across different data files.\n",
    "\n",
    "## Objectives\n",
    "1. Load and inspect all datasets (Summary Listings, Detailed Listings, Detailed Reviews, Detailed Calendar, and Summary Reviews)\n",
    "2. Handle missing values and outliers\n",
    "3. Ensure data consistency (e.g., data types, date formats)\n",
    "4. Merge relevant datasets for comprehensive analysis\n",
    "5. Create clean, analysis-ready datasets for both Paris and Los Angeles\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Initial Inspection\n",
    "2. Handling Missing Values\n",
    "3. Outlier Detection and Treatment\n",
    "4. Data Type Conversion and Consistency Checks\n",
    "5. Merging Datasets\n",
    "6. Final Data Validation and Export\n",
    "\n",
    "Let's begin by importing the necessary libraries and loading our datasets."
   ]
  },
  {
   "cell_type": "code",
   "id": "a9a39cf8318f427c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T06:11:59.714872Z",
     "start_time": "2024-10-30T06:11:59.710940Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Merge archive data into one merged file",
   "id": "fb30284a6b19588"
  },
  {
   "cell_type": "code",
   "id": "f1fe6340b889e0dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T12:00:32.817543Z",
     "start_time": "2024-10-12T12:00:14.361105Z"
    }
   },
   "source": [
    "def merge_same_csv_files(directory_path):\n",
    "    # Get all CSV files in the specified directory\n",
    "    csv_files = glob.glob(os.path.join(directory_path, \"*.csv\"))\n",
    "    \n",
    "    # Check if there are any CSV files\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {directory_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Read and store all dataframes in a list\n",
    "    dataframes = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes.append(df)\n",
    "        print(f\"Read file: {file}\")\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicate rows based on 'id' column\n",
    "    merged_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Specify the directory containing your CSV files\n",
    "file_type = \"listings_detailed\"\n",
    "directory_path = \"raw_data/\" + file_type\n",
    "\n",
    "# Merge the CSV files\n",
    "merged_data = merge_same_csv_files(directory_path)\n",
    "\n",
    "if merged_data is not None:\n",
    "    # Save the merged raw_data to a new CSV file\n",
    "    output_file = \"raw_data/merged/merged_\"+file_type+\".csv\"\n",
    "    merged_data.to_csv(output_file, index=False)\n",
    "    print(f\"Merged raw_data saved to {output_file}\")\n",
    "    print(f\"Total rows in merged file: {len(merged_data)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file: raw_data/listings_detailed\\10th_june_listing.csv\n",
      "Read file: raw_data/listings_detailed\\12_dec_2023_listing.csv\n",
      "Read file: raw_data/listings_detailed\\16_march_listing.csv\n",
      "Read file: raw_data/listings_detailed\\6th_sept_listing.csv\n",
      "Merged raw_data saved to raw_data/merged/merged_listings_detailed.csv\n",
      "Total rows in merged file: 118804\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Merge data files, such as listings and reviews",
   "id": "df2cb60239387de6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T12:03:05.261267Z",
     "start_time": "2024-10-12T12:02:54.249826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    listings = pd.read_csv('raw_data/merged/merged_listings_detailed.csv')\n",
    "    reviews = pd.read_csv('raw_data/merged/merged_reviews.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Size of listings: {listings.size}\")\n",
    "    print(f\"Size of reviews: {reviews.size}\")\n",
    "    return listings, reviews\n",
    "\n",
    "def merge_data(listings, reviews):\n",
    "    merged_data = pd.merge(listings, reviews, left_on='id', right_on='listing_id', how='left')\n",
    "    merged_data.fillna({'reviews_per_month': 0}, inplace=True)\n",
    "    print(f\"Size of merged: {merged_data.size}\")\n",
    "    return merged_data\n",
    "\n",
    "def save_data(merged_data):\n",
    "    output_file = 'raw_data/merged/merged_lst_det+rev.csv'\n",
    "    merged_data.to_csv(output_file, index=False)\n",
    "    print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "listings, reviews = load_data()\n",
    "merged_data = merge_data(listings, reviews)\n",
    "save_data(merged_data)"
   ],
   "id": "e860880ff5f9214f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjokk\\AppData\\Local\\Temp\\ipykernel_8504\\2725490828.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  listings = pd.read_csv('raw_data/merged/merged_listings_detailed.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Size of listings: 8910300\n",
      "Size of reviews: 163376\n",
      "Size of merged: 9147908\n",
      "Merged data saved to raw_data/merged/merged_lst_det+rev.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean data and set correct types",
   "id": "5f3b0949881a7f02"
  },
  {
   "cell_type": "code",
   "id": "edeccd282a3ecd0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:16:39.329138Z",
     "start_time": "2024-10-17T15:16:31.551188Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_airbnb_data(input_file, output_file, alldata=True):\n",
    "    # Read the merged CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "    # Clean the data\n",
    "    df = clean_merged_data(df, alldata)\n",
    "\n",
    "    # Save the cleaned data\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "    # Print summary of changes\n",
    "    print_summary(df)\n",
    "\n",
    "def clean_merged_data(df, alldata):\n",
    "    # Convert 'last_scraped' to datetime at the beginning\n",
    "    if 'last_scraped' in df.columns:\n",
    "        df['last_scraped'] = pd.to_datetime(df['last_scraped'], errors='coerce')\n",
    "    else:\n",
    "        print(\"Warning: 'last_scraped' column not found. Unable to remove duplicates based on most recent entry.\")\n",
    "        return df\n",
    "\n",
    "    # Remove duplicates based on 'id', keeping the most recent entry\n",
    "    original_count = len(df)\n",
    "    df = df.sort_values('last_scraped', ascending=False).drop_duplicates(subset='id', keep='first')\n",
    "    duplicate_count = original_count - len(df)\n",
    "    print(f\"Removed {duplicate_count} duplicate entries based on 'id'\")\n",
    "\n",
    "    # Function to clean price\n",
    "    def clean_price(price):\n",
    "        if pd.isna(price) or price == '':\n",
    "            return np.nan\n",
    "        return float(str(price).replace('$', '').replace(',', ''))\n",
    "\n",
    "    # Function to clean license\n",
    "    def clean_license(license):\n",
    "        if pd.isna(license) or license == '':\n",
    "            return 'Unknown'\n",
    "        return str(license)\n",
    "\n",
    "    # Clean the data\n",
    "    df['price'] = df['price'].apply(clean_price)\n",
    "    df['license'] = df['license'].apply(clean_license)\n",
    "    \n",
    "    # Convert remaining date columns to datetime\n",
    "    date_columns = ['last_review', 'date']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    # Remove rows with critical missing data\n",
    "    critical_columns = ['id', 'latitude', 'longitude', 'room_type']\n",
    "    df = df.dropna(subset=critical_columns)\n",
    "\n",
    "    # Convert id, host_id, and listing_id to int\n",
    "    for col in ['id', 'host_id', 'listing_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('Int64')  # Using Int64 to handle potential NaN values\n",
    "\n",
    "    # Fill NaN values in reviews_per_month with 0\n",
    "    if 'reviews_per_month' in df.columns:\n",
    "        df['reviews_per_month'] = df['reviews_per_month'].fillna(0)\n",
    "    \n",
    "    # Optional: remove unwanted columns if alldata is False\n",
    "    if not alldata:\n",
    "        df = remove_unwanted_columns(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_unwanted_columns(df):\n",
    "    cols_to_remove = ['listing_url', 'picture_url', 'host_url', 'host_thumbnail_url', 'host_picture_url', 'host_name', 'host_since', 'host_location','host_about', 'host_response_time','host_response_rate', 'host_acceptance_rate', 'host_neighbourhood', 'host_verifications','host_has_profile_pic', 'host_identity_verified', 'neighborhood_overview', ' neighbourhood', 'neighbourhood_group_cleansed', 'calendar_updated']\n",
    "    \n",
    "    columns_to_remove = [col for col in cols_to_remove if col in df.columns]\n",
    "    df = df.drop(columns=columns_to_remove)\n",
    "    print(f\"Removed columns: {columns_to_remove}\")\n",
    "    return df\n",
    "\n",
    "def print_summary(df):\n",
    "    print(\"\\nData Cleaning Summary:\")\n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    if 'price' in df.columns:\n",
    "        print(f\"Rows with missing prices: {df['price'].isna().sum()}\")\n",
    "    if 'license' in df.columns:\n",
    "        print(f\"Unique license values: {df['license'].nunique()}\")\n",
    "        print(f\"Sample of unique license values: {df['license'].sample(min(5, df['license'].nunique())).tolist()}\")\n",
    "    if 'date' in df.columns:\n",
    "        print(f\"Date range of reviews: {df['date'].min()} to {df['date'].max()}\")\n",
    "    if 'id' in df.columns:\n",
    "        print(f\"Number of unique listings: {df['id'].nunique()}\")\n",
    "    if 'reviews_per_month' in df.columns:\n",
    "        print(f\"Average reviews per month: {df['reviews_per_month'].mean():.2f}\")\n",
    "\n",
    "# Usage\n",
    "input_file = 'raw_data/merged/merged_listings_detailed.csv'\n",
    "output_file = 'cleaned_data/cleaned_listings_data.csv'\n",
    "clean_airbnb_data(input_file, output_file, alldata=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjokk\\AppData\\Local\\Temp\\ipykernel_11108\\1821433011.py:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (118804, 75)\n",
      "Removed 0 duplicate entries based on 'id'\n",
      "Removed columns: ['listing_url', 'picture_url', 'host_url', 'host_thumbnail_url', 'host_picture_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_neighbourhood', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'neighborhood_overview', 'neighbourhood_group_cleansed', 'calendar_updated']\n",
      "Cleaned data saved to cleaned_data/cleaned_listings_data.csv\n",
      "\n",
      "Data Cleaning Summary:\n",
      "Final shape: (118804, 56)\n",
      "Columns: ['id', 'scrape_id', 'last_scraped', 'source', 'name', 'description', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood', 'neighbourhood_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'license', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month']\n",
      "Rows with missing prices: 23371\n",
      "Unique license values: 77836\n",
      "Sample of unique license values: ['7512002132248', 'Available with a mobility lease only (\"bail mobilitÃ©\")', '7510813436840', '7509124789790', '7511602734748']\n",
      "Number of unique listings: 118804\n",
      "Average reviews per month: 0.75\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Continued cleaning",
   "id": "52245efa5bdc9b0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T16:08:48.016791Z",
     "start_time": "2024-10-17T16:08:42.778321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_file = 'cleaned_data/cleaned_listings_data.csv'\n",
    "output_file = 'cleaned_data/prepared/processed_listings_data.csv'\n",
    "\n",
    "# Read the merged CSV file\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "columns_to_remove = ['scrape_id', 'source', 'neighbourhood', 'description','license', 'bathrooms_text']\n",
    "df = df.drop(columns=columns_to_remove)\n",
    "print(f\"Removed columns: {columns_to_remove}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned data saved to {output_file}\")"
   ],
   "id": "360f39434cd2d9e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjokk\\AppData\\Local\\Temp\\ipykernel_11108\\613561500.py:5: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (118804, 56)\n",
      "Removed columns: ['scrape_id', 'source', 'neighbourhood', 'description', 'license', 'bathrooms_text']\n",
      "Cleaned data saved to cleaned_data/prepared/processed_listings_data.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plotting missing data",
   "id": "5ebaa3e20201c3e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:35:07.282660Z",
     "start_time": "2024-10-24T11:35:03.268694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the CSV data\n",
    "df = pd.read_csv('cleaned_data/prepared/processed_listings_data.csv')\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "# Sort the columns by percentage of missing values (descending order)\n",
    "missing_percentages_sorted = missing_percentages.sort_values(ascending=False)\n",
    "\n",
    "# Filter the dataframe to include only columns with missing values\n",
    "columns_with_missing = missing_percentages[missing_percentages > 0].index\n",
    "df_missing = df[columns_with_missing]\n",
    "\n",
    "# Create a heatmap of missing values with a clearer color scheme (only for columns with missing values)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_missing.isnull(), cbar=False, yticklabels=False, cmap='Reds')\n",
    "plt.title('Missing Values Heatmap (Columns with Missing Values Only)')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.savefig('missing_values_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# Create a bar plot of missing value percentages (only for columns with missing values)\n",
    "missing_percentages_filtered = missing_percentages_sorted[missing_percentages_sorted > 0]\n",
    "plt.figure(figsize=(12, 8))\n",
    "missing_percentages_filtered.plot(kind='bar')\n",
    "plt.title('Percentage of Missing Values by Column')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Percentage of Missing Values')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('missing_values_percentage.png')\n",
    "plt.close()\n",
    "\n",
    "# Print summary of missing values\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_percentages[missing_percentages > 0].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nTotal number of missing values:\", df.isnull().sum().sum())\n",
    "print(\"Total number of cells in the dataset:\", df.size)\n",
    "print(\"Percentage of missing values:\", (df.isnull().sum().sum() / df.size) * 100)\n",
    "\n",
    "# Identify columns with more than 50% missing values\n",
    "columns_to_drop = missing_percentages[missing_percentages > 50].index.tolist()\n",
    "print(\"\\nColumns with more than 50% missing values:\")\n",
    "print(columns_to_drop)\n",
    "\n",
    "# Save the list of columns to drop\n",
    "with open('columns_to_drop.txt', 'w') as f:\n",
    "    for column in columns_to_drop:\n",
    "        f.write(f\"{column}\\n\")\n",
    "\n",
    "print(\"\\nAnalysis complete. Check the generated images and 'columns_to_drop.txt' file.\")"
   ],
   "id": "58f65e447c970a61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17888\\1860922816.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;31m# Print summary of missing values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Columns with missing values:\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 39\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmissing_percentages\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m35\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mmissing_percentages\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msort_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mascending\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"\\nTotal number of missing values:\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misnull\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Total number of cells in the dataset:\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1575\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mfinal\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1576\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__nonzero__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mNoReturn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1577\u001B[1;33m         raise ValueError(\n\u001B[0m\u001B[0;32m   1578\u001B[0m             \u001B[1;33mf\"\u001B[0m\u001B[1;33mThe truth value of a \u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m is ambiguous. \u001B[0m\u001B[1;33m\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1579\u001B[0m             \u001B[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1580\u001B[0m         \u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Handle ratings",
   "id": "f7ecddb58b42e14f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T10:21:43.394365Z",
     "start_time": "2024-10-24T10:21:41.621268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_ratings_data(df):\n",
    "    \"\"\"\n",
    "    Prepare ratings data for analysis with explicit handling of unrated listings.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing Airbnb listing data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with additional rating metadata columns\n",
    "    dict: Rating statistics and segments\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Create rating status columns\n",
    "    df_clean['has_ratings'] = df_clean['number_of_reviews'] > 0\n",
    "    \n",
    "    # Create review volume segments\n",
    "    df_clean['review_volume'] = pd.cut(\n",
    "        df_clean['number_of_reviews'],\n",
    "        bins=[-np.inf, 0, 5, 20, np.inf],\n",
    "        labels=['unrated', 'few_reviews', 'moderate_reviews', 'many_reviews']\n",
    "    )\n",
    "    \n",
    "    # Calculate rating statistics only for rated listings\n",
    "    rated_listings = df_clean[df_clean['has_ratings']]\n",
    "    rating_cols = [col for col in df_clean.columns if col.startswith('review_scores_')]\n",
    "    \n",
    "    stats = {\n",
    "        'segments': {\n",
    "            'total_listings': len(df_clean),\n",
    "            'rated_listings': len(rated_listings),\n",
    "            'unrated_listings': len(df_clean) - len(rated_listings),\n",
    "            'review_volume_distribution': df_clean['review_volume'].value_counts().to_dict()\n",
    "        },\n",
    "        'rating_stats': {\n",
    "            col: {\n",
    "                'mean': rated_listings[col].mean(),\n",
    "                'median': rated_listings[col].median(),\n",
    "                'std': rated_listings[col].std(),\n",
    "                'q25': rated_listings[col].quantile(0.25),\n",
    "                'q75': rated_listings[col].quantile(0.75)\n",
    "            } for col in rating_cols\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return df_clean, stats\n",
    "\n",
    "def get_rating_subset(df, min_reviews=None, include_unrated=False):\n",
    "    \"\"\"\n",
    "    Get a subset of listings based on rating criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Prepared DataFrame from prepare_ratings_data\n",
    "    min_reviews (int, optional): Minimum number of reviews required\n",
    "    include_unrated (bool): Whether to include unrated listings\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    if not include_unrated:\n",
    "        mask &= df['has_ratings']\n",
    "    \n",
    "    if min_reviews is not None:\n",
    "        mask &= df['number_of_reviews'] >= min_reviews\n",
    "    \n",
    "    return df[mask]\n",
    "\n",
    "def print_rating_summary(stats):\n",
    "    \"\"\"Print a summary of the rating statistics.\"\"\"\n",
    "    print(\"=== Rating Data Summary ===\")\n",
    "    print(f\"\\nSegment Sizes:\")\n",
    "    print(f\"Total listings: {stats['segments']['total_listings']:,}\")\n",
    "    print(f\"Rated listings: {stats['segments']['rated_listings']:,}\")\n",
    "    print(f\"Unrated listings: {stats['segments']['unrated_listings']:,}\")\n",
    "    \n",
    "    print(\"\\nReview Volume Distribution:\")\n",
    "    for segment, count in stats['segments']['review_volume_distribution'].items():\n",
    "        print(f\"  {segment}: {count:,}\")\n",
    "    \n",
    "    print(\"\\nRating Statistics (rated listings only):\")\n",
    "    for col, metrics in stats['rating_stats'].items():\n",
    "        print(f\"\\n{col}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        \n",
    "        \n",
    "# Read your data\n",
    "df = pd.read_csv('cleaned_data/prepared/processed_listings_data.csv')\n",
    "# Prepare the data\n",
    "df_prepared, stats = prepare_ratings_data(df)\n",
    "\n",
    "print_rating_summary(stats)\n",
    "\n",
    "# For visualizations of ratings, use only rated listings\n",
    "rated_df = get_rating_subset(df_prepared, include_unrated=False)\n",
    "\n",
    "# For analyses that need high-confidence ratings\n",
    "high_confidence_df = get_rating_subset(df_prepared, min_reviews=5)\n",
    "\n",
    "# For full market analysis, use all listings but handle ratings explicitly\n",
    "all_df = get_rating_subset(df_prepared, include_unrated=True)"
   ],
   "id": "6a0854cbc45d7023",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Rating Data Summary ===\n",
      "\n",
      "Segment Sizes:\n",
      "Total listings: 118,804\n",
      "Rated listings: 76,235\n",
      "Unrated listings: 42,569\n",
      "\n",
      "Review Volume Distribution:\n",
      "  unrated: 42,569\n",
      "  few_reviews: 29,208\n",
      "  many_reviews: 23,874\n",
      "  moderate_reviews: 23,153\n",
      "\n",
      "Rating Statistics (rated listings only):\n",
      "\n",
      "review_scores_rating:\n",
      "  mean: 4.69\n",
      "  median: 4.83\n",
      "  std: 0.46\n",
      "  q25: 4.57\n",
      "  q75: 5.00\n",
      "\n",
      "review_scores_accuracy:\n",
      "  mean: 4.74\n",
      "  median: 4.88\n",
      "  std: 0.43\n",
      "  q25: 4.67\n",
      "  q75: 5.00\n",
      "\n",
      "review_scores_cleanliness:\n",
      "  mean: 4.62\n",
      "  median: 4.77\n",
      "  std: 0.51\n",
      "  q25: 4.50\n",
      "  q75: 5.00\n",
      "\n",
      "review_scores_checkin:\n",
      "  mean: 4.79\n",
      "  median: 4.92\n",
      "  std: 0.42\n",
      "  q25: 4.75\n",
      "  q75: 5.00\n",
      "\n",
      "review_scores_communication:\n",
      "  mean: 4.81\n",
      "  median: 4.96\n",
      "  std: 0.40\n",
      "  q25: 4.78\n",
      "  q75: 5.00\n",
      "\n",
      "review_scores_location:\n",
      "  mean: 4.80\n",
      "  median: 4.92\n",
      "  std: 0.35\n",
      "  q25: 4.75\n",
      "  q75: 5.00\n",
      "\n",
      "review_scores_value:\n",
      "  mean: 4.60\n",
      "  median: 4.71\n",
      "  std: 0.49\n",
      "  q25: 4.49\n",
      "  q75: 4.92\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Handle accomodation features",
   "id": "1b38db4aa5ea27d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:32:52.122550Z",
     "start_time": "2024-10-24T11:32:46.084224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def impute_accommodation_features(df):\n",
    "    \"\"\"\n",
    "    Impute missing bedroom and bed values using logical rules and accommodates as proxy.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing Airbnb listing data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with imputed values\n",
    "    dict: Imputation statistics\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Track imputation methods\n",
    "    imputation_stats = {\n",
    "        'bedrooms': {'single_room_rule': 0, 'accommodates_proxy': 0},\n",
    "        'beds': {'single_room_rule': 0, 'accommodates_proxy': 0}\n",
    "    }\n",
    "    \n",
    "    # Step 1: Apply single room rule\n",
    "    single_room_types = ['Private room', 'Shared room']\n",
    "    for feature in ['bedrooms', 'beds']:\n",
    "        mask = (df_clean[feature].isna()) & (df_clean['room_type'].isin(single_room_types))\n",
    "        df_clean.loc[mask, feature] = 1\n",
    "        imputation_stats[feature]['single_room_rule'] = mask.sum()\n",
    "    \n",
    "    # Step 2: Use accommodates as proxy for remaining missing values\n",
    "    for feature in ['bedrooms', 'beds']:\n",
    "        # Calculate mean values by accommodates\n",
    "        means_by_accommodates = df_clean.groupby('accommodates')[feature].mean()\n",
    "        \n",
    "        # Fill remaining missing values\n",
    "        mask = df_clean[feature].isna()\n",
    "        df_clean.loc[mask, feature] = df_clean.loc[mask, 'accommodates'].map(means_by_accommodates)\n",
    "        imputation_stats[feature]['accommodates_proxy'] = mask.sum()\n",
    "    \n",
    "    # Step 3: Apply logical constraints\n",
    "    # Ensure beds >= bedrooms\n",
    "    df_clean['beds'] = df_clean.apply(\n",
    "        lambda x: max(x['beds'], x['bedrooms']) if pd.notnull(x['bedrooms']) else x['beds'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add imputation flags\n",
    "    df_clean['bedrooms_imputed_method'] = 'original'\n",
    "    mask = df_clean['bedrooms'] != df['bedrooms']\n",
    "    df_clean.loc[mask & (df_clean['room_type'].isin(single_room_types)), 'bedrooms_imputed_method'] = 'single_room_rule'\n",
    "    df_clean.loc[mask & (~df_clean['room_type'].isin(single_room_types)), 'bedrooms_imputed_method'] = 'accommodates_proxy'\n",
    "    \n",
    "    df_clean['beds_imputed_method'] = 'original'\n",
    "    mask = df_clean['beds'] != df['beds']\n",
    "    df_clean.loc[mask & (df_clean['room_type'].isin(single_room_types)), 'beds_imputed_method'] = 'single_room_rule'\n",
    "    df_clean.loc[mask & (~df_clean['room_type'].isin(single_room_types)), 'beds_imputed_method'] = 'accommodates_proxy'\n",
    "    \n",
    "    return df_clean, imputation_stats\n",
    "\n",
    "def print_imputation_summary(df_original, df_imputed, stats):\n",
    "    \"\"\"Print summary of the imputation process and results.\"\"\"\n",
    "    print(\"=== Imputation Summary ===\\n\")\n",
    "    \n",
    "    # Missing values before\n",
    "    print(\"Missing Values Before Imputation:\")\n",
    "    print(f\"Bedrooms: {df_original['bedrooms'].isna().sum():,}\")\n",
    "    print(f\"Beds: {df_original['beds'].isna().sum():,}\\n\")\n",
    "    \n",
    "    # Imputation methods used\n",
    "    print(\"Imputation Methods Used:\")\n",
    "    for feature, methods in stats.items():\n",
    "        print(f\"\\n{feature.title()}:\")\n",
    "        for method, count in methods.items():\n",
    "            print(f\"  {method}: {count:,}\")\n",
    "    \n",
    "    # Distribution comparison\n",
    "    print(\"\\nValue Distribution After Imputation:\")\n",
    "    for feature in ['bedrooms', 'beds']:\n",
    "        print(f\"\\n{feature.title()}:\")\n",
    "        print(df_imputed[feature].value_counts().sort_index().head())\n",
    "    \n",
    "    # Validation checks\n",
    "    print(\"\\nValidation Checks:\")\n",
    "    print(f\"Beds >= Bedrooms: {(df_imputed['beds'] >= df_imputed['bedrooms']).mean()*100:.2f}%\")\n",
    "    print(f\"Single rooms with 1 bedroom: {(df_imputed[df_imputed['room_type'].isin(['Private room', 'Shared room'])]['bedrooms'] == 1).mean()*100:.2f}%\")\n",
    "    \n",
    "df = pd.read_csv('cleaned_data/prepared/processed_listings_data.csv')\n",
    "# Apply the imputation\n",
    "df_cleaned, imputation_stats = impute_accommodation_features(df)\n",
    "\n",
    "# Print the summary\n",
    "print_imputation_summary(df, df_cleaned, imputation_stats)\n",
    "\n",
    "# Save the cleaned data\n",
    "df_cleaned.to_csv(\"cleaned_data/prepared/acc_processed_listings_data.csv\", index=False)"
   ],
   "id": "1d505cb119f692ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Imputation Summary ===\n",
      "\n",
      "Missing Values Before Imputation:\n",
      "Bedrooms: 17,627\n",
      "Beds: 22,810\n",
      "\n",
      "Imputation Methods Used:\n",
      "\n",
      "Bedrooms:\n",
      "  single_room_rule: 4,059\n",
      "  accommodates_proxy: 13,568\n",
      "\n",
      "Beds:\n",
      "  single_room_rule: 3,474\n",
      "  accommodates_proxy: 19,336\n",
      "\n",
      "Value Distribution After Imputation:\n",
      "\n",
      "Bedrooms:\n",
      "bedrooms\n",
      "0.000000     8561\n",
      "0.876278     7574\n",
      "0.934118      427\n",
      "1.000000    67026\n",
      "1.120280     1152\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Beds:\n",
      "beds\n",
      "0.000000    1532\n",
      "0.876278      13\n",
      "0.911110    3128\n",
      "0.934118       2\n",
      "0.972815     237\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation Checks:\n",
      "Beds >= Bedrooms: 100.00%\n",
      "Single rooms with 1 bedroom: 90.54%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 97\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# Save the cleaned data\u001B[39;00m\n\u001B[0;32m     96\u001B[0m df_cleaned\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcleaned_data/prepared/acc_processed_listings_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 97\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCleaned data saved to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'output_file' is not defined"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyzing bathrooms",
   "id": "fd17ad1a6cf69129"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T12:38:24.121498Z",
     "start_time": "2024-10-24T12:38:21.711436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def analyze_missing_bathrooms(df):\n",
    "    \"\"\"\n",
    "    Analyze patterns of missing bathroom data across different room types\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame with Airbnb listing data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Basic missing data analysis by room type\n",
    "    missing_by_room = data.groupby('room_type').agg({\n",
    "        'bathrooms': lambda x: x.isna().sum(),\n",
    "        'id': 'count'  # Total count using ID column\n",
    "    }).rename(columns={\n",
    "        'bathrooms': 'missing_bathrooms',\n",
    "        'id': 'total_listings'\n",
    "    })\n",
    "    \n",
    "    # Calculate percentages\n",
    "    missing_by_room['missing_percentage'] = (\n",
    "        missing_by_room['missing_bathrooms'] / missing_by_room['total_listings'] * 100\n",
    "    ).round(2)\n",
    "    \n",
    "    # Property type and room type combination analysis\n",
    "    missing_by_prop_room = data.groupby(['property_type', 'room_type']).agg({\n",
    "        'bathrooms': lambda x: x.isna().sum(),\n",
    "        'id': 'count'\n",
    "    }).rename(columns={\n",
    "        'bathrooms': 'missing_bathrooms',\n",
    "        'id': 'total_listings'\n",
    "    }).reset_index()\n",
    "    \n",
    "    missing_by_prop_room['missing_percentage'] = (\n",
    "        missing_by_prop_room['missing_bathrooms'] / missing_by_prop_room['total_listings'] * 100\n",
    "    ).round(2)\n",
    "    \n",
    "    # Get top 10 property types with highest missing percentages\n",
    "    # Only consider combinations with at least 10 listings\n",
    "    top_missing = (\n",
    "        missing_by_prop_room[missing_by_prop_room['total_listings'] >= 10]\n",
    "        .sort_values('missing_percentage', ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    \n",
    "    # Analyze if missing bathrooms correlate with other features\n",
    "    correlation_data = data.copy()\n",
    "    correlation_data['has_bathroom'] = correlation_data['bathrooms'].notna().astype(int)\n",
    "    \n",
    "    # Select relevant numeric columns for correlation\n",
    "    numeric_cols = []\n",
    "    for col in ['accommodates', 'bedrooms', 'beds', 'price', \n",
    "                'minimum_nights', 'number_of_reviews', 'has_bathroom']:\n",
    "        if col in correlation_data.columns:\n",
    "            # Convert to numeric if possible\n",
    "            correlation_data[col] = pd.to_numeric(\n",
    "                correlation_data[col].replace('[\\$,]', '', regex=True),\n",
    "                errors='coerce'\n",
    "            )\n",
    "            numeric_cols.append(col)\n",
    "    \n",
    "    correlation_matrix = correlation_data[numeric_cols].corr()['has_bathroom'].sort_values()\n",
    "    \n",
    "    results = {\n",
    "        'room_type_summary': missing_by_room,\n",
    "        'property_room_details': missing_by_prop_room,\n",
    "        'top_missing_combinations': top_missing,\n",
    "        'feature_correlations': correlation_matrix,\n",
    "        'total_listings': len(data),\n",
    "        'total_missing': data['bathrooms'].isna().sum(),\n",
    "        'overall_missing_percentage': (data['bathrooms'].isna().sum() / len(data) * 100).round(2)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_missing_bathroom_analysis(results):\n",
    "    \"\"\"\n",
    "    Create visualizations for missing bathroom analysis\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Missing bathrooms by room type\n",
    "    room_stats = results['room_type_summary']\n",
    "    bars = ax1.bar(\n",
    "        range(len(room_stats.index)),\n",
    "        room_stats['missing_percentage']\n",
    "    )\n",
    "    ax1.set_xticks(range(len(room_stats.index)))\n",
    "    ax1.set_xticklabels(room_stats.index, rotation=45)\n",
    "    ax1.set_title('Percentage of Missing Bathrooms by Room Type')\n",
    "    ax1.set_ylabel('Missing Percentage')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "        )\n",
    "    \n",
    "    # Plot 2: Top 10 property-room combinations with missing bathrooms\n",
    "    top_missing = results['top_missing_combinations']\n",
    "    labels = [f\"{prop}\\n({room})\" for prop, room in \n",
    "             zip(top_missing['property_type'], top_missing['room_type'])]\n",
    "    \n",
    "    bars = ax2.bar(range(len(labels)), top_missing['missing_percentage'])\n",
    "    ax2.set_xticks(range(len(labels)))\n",
    "    ax2.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax2.set_title('Top Property-Room Combinations\\nwith Missing Bathrooms')\n",
    "    ax2.set_ylabel('Missing Percentage')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def save_room_type_plot(results, filename='missing_bathrooms_by_room_type.png', dpi=300):\n",
    "    \"\"\"\n",
    "    Create and save a bar plot showing percentage of missing bathrooms by room type\n",
    "    \n",
    "    Parameters:\n",
    "    results: dict containing analysis results\n",
    "    filename: str, name of file to save (default: 'missing_bathrooms_by_room_type.png')\n",
    "    dpi: int, resolution of saved image (default: 300)\n",
    "    \"\"\"\n",
    "    # Create figure and axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get room type statistics\n",
    "    room_stats = results['room_type_summary']\n",
    "    \n",
    "    # Create bars\n",
    "    bars = plt.bar(\n",
    "        range(len(room_stats.index)),\n",
    "        room_stats['missing_percentage']\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xticks(range(len(room_stats.index)), room_stats.index, rotation=45)\n",
    "    plt.title('Percentage of Missing Bathrooms by Room Type', pad=20)\n",
    "    plt.ylabel('Missing Percentage')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "        )\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Usage example:\n",
    "# results = analyze_missing_bathrooms(df)\n",
    "# save_room_type_plot(results)\n",
    "\n",
    "df = pd.read_csv('cleaned_data/prepared/acc_processed_listings_data.csv')\n",
    "# Assuming your DataFrame is called 'airbnb_df'\n",
    "results = analyze_missing_bathrooms(df)\n",
    "\n",
    "# Print overall statistics\n",
    "print(f\"Total listings analyzed: {results['total_listings']}\")\n",
    "print(f\"Total missing bathroom values: {results['total_missing']}\")\n",
    "print(f\"Overall missing percentage: {results['overall_missing_percentage']}%\\n\")\n",
    "\n",
    "print(\"Missing Bathrooms by Room Type:\")\n",
    "print(results['room_type_summary'])\n",
    "\n",
    "print(\"\\nTop 10 Property-Room Combinations with Highest Missing Percentages:\")\n",
    "print(results['top_missing_combinations'][['property_type', 'room_type', 'missing_percentage']])\n",
    "\n",
    "# Create visualizations\n",
    "save_room_type_plot(results, \"visualizations/missing_bathrooms_by_room_type.png\")"
   ],
   "id": "ab6ec776d9cfd14d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:65: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:65: SyntaxWarning: invalid escape sequence '\\$'\n",
      "C:\\Users\\sjokk\\AppData\\Local\\Temp\\ipykernel_17888\\2260720592.py:65: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  correlation_data[col].replace('[\\$,]', '', regex=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total listings analyzed: 118804\n",
      "Total missing bathroom values: 33788\n",
      "Overall missing percentage: 28.44%\n",
      "\n",
      "Missing Bathrooms by Room Type:\n",
      "                 missing_bathrooms  total_listings  missing_percentage\n",
      "room_type                                                             \n",
      "Entire home/apt              28513          106003               26.90\n",
      "Hotel room                     279             834               33.45\n",
      "Private room                  4704           11375               41.35\n",
      "Shared room                    292             592               49.32\n",
      "\n",
      "Top 10 Property-Room Combinations with Highest Missing Percentages:\n",
      "                  property_type        room_type  missing_percentage\n",
      "46           Room in aparthotel     Private room               76.09\n",
      "47    Room in bed and breakfast       Hotel room               71.43\n",
      "65        Shared room in hostel      Shared room               60.96\n",
      "69   Shared room in rental unit      Shared room               47.67\n",
      "37  Private room in rental unit     Private room               47.31\n",
      "32   Private room in guesthouse     Private room               45.10\n",
      "60         Shared room in condo      Shared room               42.31\n",
      "29        Private room in condo     Private room               42.02\n",
      "36         Private room in loft     Private room               40.48\n",
      "1                          Boat  Entire home/apt               40.00\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fixing bathrooms",
   "id": "717371acb7564d2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T12:50:21.101058Z",
     "start_time": "2024-10-24T12:50:16.201399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def impute_bathrooms(df):\n",
    "    \"\"\"\n",
    "    Impute bathroom values for hotel rooms and private rooms while preserving\n",
    "    other room types to maintain data integrity.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame with Airbnb listing data\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame: Copy of input data with imputed bathroom values\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Store original missing counts for reporting\n",
    "    original_missing = data['bathrooms'].isna().sum()\n",
    "    \n",
    "    # Create mask for rooms we want to impute\n",
    "    hotel_mask = (data['property_type'] == 'Hotel') & (data['bathrooms'].isna())\n",
    "    private_room_mask = (data['room_type'] == 'Private room') & (data['bathrooms'].isna())\n",
    "    \n",
    "    # Impute values\n",
    "    data.loc[hotel_mask, 'bathrooms'] = 1.0\n",
    "    data.loc[private_room_mask, 'bathrooms'] = 1.0\n",
    "    \n",
    "    # Calculate imputation statistics\n",
    "    imputed_count = hotel_mask.sum() + private_room_mask.sum()\n",
    "    remaining_missing = data['bathrooms'].isna().sum()\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary = {\n",
    "        'original_missing': original_missing,\n",
    "        'imputed_count': imputed_count,\n",
    "        'remaining_missing': remaining_missing,\n",
    "        'imputed_percentage': (imputed_count / original_missing * 100).round(2) if original_missing > 0 else 0,\n",
    "        'hotel_rooms_imputed': hotel_mask.sum(),\n",
    "        'private_rooms_imputed': private_room_mask.sum()\n",
    "    }\n",
    "    \n",
    "    # Add imputation method column if you want to track how values were imputed\n",
    "    data['bathrooms_imputed_method'] = np.where(hotel_mask, 'hotel_rule',\n",
    "                                              np.where(private_room_mask, 'private_room_rule',\n",
    "                                                      'original'))\n",
    "    \n",
    "    return data, summary\n",
    "\n",
    "\n",
    "df = pd.read_csv('cleaned_data/prepared/acc_processed_listings_data.csv')\n",
    "df_imputed, imputation_summary = impute_bathrooms(df)\n",
    "\n",
    "print(f\"Original missing values: {imputation_summary['original_missing']}\")\n",
    "print(f\"Values imputed: {imputation_summary['imputed_count']}\")\n",
    "print(f\"Remaining missing values: {imputation_summary['remaining_missing']}\")\n",
    "print(f\"Percentage of missing values imputed: {imputation_summary['imputed_percentage']}%\")\n",
    "print(f\"Hotel rooms imputed: {imputation_summary['hotel_rooms_imputed']}\")\n",
    "print(f\"Private rooms imputed: {imputation_summary['private_rooms_imputed']}\")\n",
    "\n",
    "df_cleaned.to_csv(\"cleaned_data/prepared/acc_bath_processed_listings_data.csv\", index=False)"
   ],
   "id": "93f9353c8fa5f5fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original missing values: 33788\n",
      "Values imputed: 4704\n",
      "Remaining missing values: 29084\n",
      "Percentage of missing values imputed: 13.92%\n",
      "Hotel rooms imputed: 0\n",
      "Private rooms imputed: 4704\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fix empty host entries",
   "id": "961ceb0875491755"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T12:57:31.055034Z",
     "start_time": "2024-10-24T12:57:26.135152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_host_and_availability_data(df):\n",
    "    \"\"\"\n",
    "    Clean and fix missing or problematic values in host and availability related columns\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame with Airbnb listing data\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame: Copy of input data with cleaned values\n",
    "    dict: Summary of changes made\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Store original missing counts\n",
    "    original_missing = {\n",
    "        'host_is_superhost': data['host_is_superhost'].isna().sum(),\n",
    "        'has_availability': data['has_availability'].isna().sum(),\n",
    "        'host_total_listings_count': data['host_total_listings_count'].isna().sum(),\n",
    "        'host_listings_count': data['host_listings_count'].isna().sum()\n",
    "    }\n",
    "    \n",
    "    # 1. Fix host_is_superhost\n",
    "    # Convert to boolean representation (f -> False, t -> True, empty -> False)\n",
    "    data['host_is_superhost'] = data['host_is_superhost'].fillna('f')\n",
    "    data['host_is_superhost'] = (data['host_is_superhost'] == 't')\n",
    "    \n",
    "    # 2. Fix has_availability\n",
    "    # If availability_30 > 0, then has_availability should be true\n",
    "    data['has_availability'] = data['has_availability'].fillna('')\n",
    "    data.loc[data['availability_30'] > 0, 'has_availability'] = 't'\n",
    "    data.loc[data['availability_30'] == 0, 'has_availability'] = 'f'\n",
    "    \n",
    "    # 3. Fix host listing counts\n",
    "    # If one is missing but the other isn't, use the non-missing value\n",
    "    data['host_total_listings_count'] = data['host_total_listings_count'].fillna(data['host_listings_count'])\n",
    "    data['host_listings_count'] = data['host_listings_count'].fillna(data['host_total_listings_count'])\n",
    "    \n",
    "    # If both are missing, use calculated values if available\n",
    "    calc_total = (data['calculated_host_listings_count_entire_homes'] + \n",
    "                 data['calculated_host_listings_count_private_rooms'] + \n",
    "                 data['calculated_host_listings_count_shared_rooms'])\n",
    "    \n",
    "    mask_missing_both = (data['host_total_listings_count'].isna() & \n",
    "                        data['host_listings_count'].isna())\n",
    "    \n",
    "    data.loc[mask_missing_both, 'host_total_listings_count'] = calc_total[mask_missing_both]\n",
    "    data.loc[mask_missing_both, 'host_listings_count'] = calc_total[mask_missing_both]\n",
    "    \n",
    "    # If still missing, set to 1 (assuming single listing)\n",
    "    data['host_total_listings_count'] = data['host_total_listings_count'].fillna(1)\n",
    "    data['host_listings_count'] = data['host_listings_count'].fillna(1)\n",
    "    \n",
    "    # Calculate summary of changes\n",
    "    final_missing = {\n",
    "        'host_is_superhost': data['host_is_superhost'].isna().sum(),\n",
    "        'has_availability': data['has_availability'].isna().sum(),\n",
    "        'host_total_listings_count': data['host_total_listings_count'].isna().sum(),\n",
    "        'host_listings_count': data['host_listings_count'].isna().sum()\n",
    "    }\n",
    "    \n",
    "    summary = {\n",
    "        'original_missing': original_missing,\n",
    "        'final_missing': final_missing,\n",
    "        'changes': {\n",
    "            col: original_missing[col] - final_missing[col] \n",
    "            for col in original_missing.keys()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return data, summary\n",
    "\n",
    "# Read your data\n",
    "df = pd.read_csv('cleaned_data/prepared/acc_bath_processed_listings_data.csv')\n",
    "\n",
    "# Apply the cleaning\n",
    "df_cleaned, summary = clean_host_and_availability_data(df)\n",
    "\n",
    "# Print summary of changes\n",
    "print(\"\\nCleaning Summary:\")\n",
    "for col in summary['changes']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Original missing: {summary['original_missing'][col]}\")\n",
    "    print(f\"  Values fixed: {summary['changes'][col]}\")\n",
    "    print(f\"  Remaining missing: {summary['final_missing'][col]}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "df_cleaned.to_csv('cleaned_data/prepared/final_processed_listings_data.csv', index=False)"
   ],
   "id": "e4ba678369a9f250",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Summary:\n",
      "\n",
      "host_is_superhost:\n",
      "  Original missing: 398\n",
      "  Values fixed: 398\n",
      "  Remaining missing: 0\n",
      "\n",
      "has_availability:\n",
      "  Original missing: 6380\n",
      "  Values fixed: 6380\n",
      "  Remaining missing: 0\n",
      "\n",
      "host_total_listings_count:\n",
      "  Original missing: 10\n",
      "  Values fixed: 10\n",
      "  Remaining missing: 0\n",
      "\n",
      "host_listings_count:\n",
      "  Original missing: 10\n",
      "  Values fixed: 10\n",
      "  Remaining missing: 0\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Resulting missing values",
   "id": "399d0a36f681a615"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T12:58:39.334263Z",
     "start_time": "2024-10-24T12:58:36.878839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the CSV data\n",
    "df = pd.read_csv('cleaned_data/prepared/final_processed_listings_data.csv')\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "# Sort the columns by percentage of missing values (descending order)\n",
    "missing_percentages_sorted = missing_percentages.sort_values(ascending=False)\n",
    "\n",
    "# Filter to include only columns with missing values BELOW 35%\n",
    "columns_with_missing = missing_percentages[(missing_percentages > 0) & (missing_percentages <= 35)].index\n",
    "df_missing = df[columns_with_missing]\n",
    "\n",
    "# Create a heatmap of missing values with a clearer color scheme\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_missing.isnull(), cbar=False, yticklabels=False, cmap='Reds')\n",
    "plt.title('Missing Values Heatmap (Columns with 1-35% Missing Values)')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.savefig('missing_values_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# Create a bar plot of missing value percentages\n",
    "missing_percentages_filtered = missing_percentages_sorted[(missing_percentages_sorted > 0) & (missing_percentages_sorted <= 35)]\n",
    "plt.figure(figsize=(12, 8))\n",
    "missing_percentages_filtered.plot(kind='bar')\n",
    "plt.title('Percentage of Missing Values by Column (1-35%)')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Percentage of Missing Values')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('missing_values_percentage.png')\n",
    "plt.close()\n",
    "\n",
    "# Print summary of missing values\n",
    "print(\"Columns with missing values (1-35%):\")\n",
    "print(missing_percentages[(missing_percentages > 0) & (missing_percentages <= 35)].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nTotal number of missing values in filtered columns:\", df_missing.isnull().sum().sum())\n",
    "print(\"Total number of cells in filtered dataset:\", df_missing.size)\n",
    "print(\"Percentage of missing values in filtered dataset:\", (df_missing.isnull().sum().sum() / df_missing.size) * 100)\n",
    "\n",
    "# Identify columns with more than 35% missing values\n",
    "columns_to_drop = missing_percentages[missing_percentages > 35].index.tolist()\n",
    "print(\"\\nColumns with more than 35% missing values (excluded from visualization):\")\n",
    "print(columns_to_drop)\n",
    "\n",
    "# Save the list of columns to drop\n",
    "with open('columns_to_drop.txt', 'w') as f:\n",
    "    for column in columns_to_drop:\n",
    "        f.write(f\"{column}\\n\")\n",
    "\n",
    "print(\"\\nAnalysis complete. Check the generated images and 'columns_to_drop.txt' file.\")"
   ],
   "id": "1e092cdb04a4c449",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values (1-35%):\n",
      "bathrooms    28.440120\n",
      "price        19.671897\n",
      "dtype: float64\n",
      "\n",
      "Total number of missing values in filtered columns: 57159\n",
      "Total number of cells in filtered dataset: 237608\n",
      "Percentage of missing values in filtered dataset: 24.05600821521161\n",
      "\n",
      "Columns with more than 35% missing values (excluded from visualization):\n",
      "['first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value']\n",
      "\n",
      "Analysis complete. Check the generated images and 'columns_to_drop.txt' file.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T13:04:56.502086Z",
     "start_time": "2024-10-24T13:04:54.106197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_listing_counts(df):\n",
    "    \"\"\"\n",
    "    Analyze differences between host_total_listings_count and host_listings_count\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame with Airbnb listing data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Convert columns to numeric if they aren't already\n",
    "    data['host_total_listings_count'] = pd.to_numeric(data['host_total_listings_count'], errors='coerce')\n",
    "    data['host_listings_count'] = pd.to_numeric(data['host_listings_count'], errors='coerce')\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    total_rows = len(data)\n",
    "    matching_rows = (data['host_total_listings_count'] == data['host_listings_count']).sum()\n",
    "    different_rows = total_rows - matching_rows\n",
    "    \n",
    "    # Calculate differences where they exist\n",
    "    data['listing_count_diff'] = data['host_total_listings_count'] - data['host_listings_count']\n",
    "    \n",
    "    # Get statistics about differences\n",
    "    diff_stats = data['listing_count_diff'].describe()\n",
    "    \n",
    "    # Find examples of largest differences\n",
    "    largest_diff = (\n",
    "        data[data['listing_count_diff'] != 0]\n",
    "        .sort_values('listing_count_diff', ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'total_listings': total_rows,\n",
    "        'matching_counts': matching_rows,\n",
    "        'different_counts': different_rows,\n",
    "        'matching_percentage': (matching_rows / total_rows * 100).round(2),\n",
    "        'difference_stats': diff_stats,\n",
    "        'largest_differences': largest_diff\n",
    "    }\n",
    "    \n",
    "    return results, data['listing_count_diff']\n",
    "\n",
    "def plot_listing_count_differences(diff_series):\n",
    "    \"\"\"\n",
    "    Create visualizations of the differences between listing counts\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create histogram of differences, excluding zeros\n",
    "    non_zero_diff = diff_series[diff_series != 0]\n",
    "    plt.hist(non_zero_diff, bins=50, edgecolor='black')\n",
    "    plt.title('Distribution of Differences Between\\nTotal Listings Count and Listings Count')\n",
    "    plt.xlabel('Difference (Total - Regular)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv('cleaned_data/prepared/final_processed_listings_data.csv')\n",
    "results, diff_series = analyze_listing_counts(df)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nListing Count Comparison Summary:\")\n",
    "print(f\"Total listings analyzed: {results['total_listings']:,}\")\n",
    "print(f\"Listings with matching counts: {results['matching_counts']:,} ({results['matching_percentage']}%)\")\n",
    "print(f\"Listings with different counts: {results['different_counts']:,} ({100-results['matching_percentage']:.2f}%)\")\n",
    "\n",
    "print(\"\\nDifference Statistics:\")\n",
    "print(results['difference_stats'])\n",
    "\n",
    "print(\"\\nTop 10 Largest Differences:\")\n",
    "print(results['largest_differences'][['host_id', 'host_total_listings_count', \n",
    "                                    'host_listings_count', 'listing_count_diff']])\n",
    "\n",
    "# Create and save visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_listing_count_differences(diff_series)\n",
    "plt.savefig('listing_count_differences.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ],
   "id": "359bce9522a4eeb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing Count Comparison Summary:\n",
      "Total listings analyzed: 118,804\n",
      "Listings with matching counts: 71,550 (60.23%)\n",
      "Listings with different counts: 47,254 (39.77%)\n",
      "\n",
      "Difference Statistics:\n",
      "count    118804.000000\n",
      "mean          9.498872\n",
      "std          67.009717\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           2.000000\n",
      "max        3748.000000\n",
      "Name: listing_count_diff, dtype: float64\n",
      "\n",
      "Top 10 Largest Differences:\n",
      "         host_id  host_total_listings_count  host_listings_count  \\\n",
      "99805  439074505                     6278.0               2530.0   \n",
      "84879  439074505                     6278.0               2530.0   \n",
      "14186  439074505                     6278.0               2530.0   \n",
      "89734  439074505                     6278.0               2530.0   \n",
      "29063  439074505                     6278.0               2530.0   \n",
      "29019  439074505                     6278.0               2530.0   \n",
      "16089  439074505                     6278.0               2530.0   \n",
      "50639  402191311                     1631.0                366.0   \n",
      "80319  402191311                     1631.0                366.0   \n",
      "80355  402191311                     1631.0                366.0   \n",
      "\n",
      "       listing_count_diff  \n",
      "99805              3748.0  \n",
      "84879              3748.0  \n",
      "14186              3748.0  \n",
      "89734              3748.0  \n",
      "29063              3748.0  \n",
      "29019              3748.0  \n",
      "16089              3748.0  \n",
      "50639              1265.0  \n",
      "80319              1265.0  \n",
      "80355              1265.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Remove the faulty price column",
   "id": "9f884f84379a37d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T06:12:09.709226Z",
     "start_time": "2024-10-30T06:12:04.686046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the main dataset\n",
    "main_df = pd.read_csv('cleaned_data/prepared/final_processed_listings_data.csv')\n",
    "\n",
    "# Show the current columns (optional, for verification)\n",
    "print(\"Current columns:\", main_df.columns.tolist())\n",
    "\n",
    "# Remove existing price column\n",
    "if 'price' in main_df.columns:\n",
    "    main_df = main_df.drop(columns=['price'])\n",
    "    print(\"\\nRemoved existing 'price' column\")\n",
    "\n",
    "# Save back to the same file\n",
    "main_df.to_csv('final_processed_listings_data.csv', index=False)\n",
    "print(f\"Saved dataset with {len(main_df)} rows and {len(main_df.columns)} columns\")\n",
    "\n",
    "# Verify the column was removed (optional)\n",
    "print(\"\\nRemaining columns:\", main_df.columns.tolist())"
   ],
   "id": "fd8052a7be73fd40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current columns: ['id', 'last_scraped', 'name', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month', 'bedrooms_imputed_method', 'beds_imputed_method']\n",
      "\n",
      "Removed existing 'price' column\n",
      "Saved dataset with 118804 rows and 51 columns\n",
      "\n",
      "Remaining columns: ['id', 'last_scraped', 'name', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'amenities', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month', 'bedrooms_imputed_method', 'beds_imputed_method']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T07:00:41.216223Z",
     "start_time": "2024-10-30T07:00:36.253170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the main dataset\n",
    "main_df = pd.read_csv('cleaned_data/prepared/final_processed_listings_data.csv')\n",
    "\n",
    "# Show the current columns (optional, for verification)\n",
    "print(\"Current columns:\", main_df.columns.tolist())\n",
    "\n",
    "main_df = main_df.drop(columns=['beds_imputed_method', 'bedrooms_imputed_method', 'calendar_last_scraped'])\n",
    "\n",
    "# Save back to the same file\n",
    "main_df.to_csv('final_processed_listings_data.csv', index=False)\n",
    "print(f\"Saved dataset with {len(main_df)} rows and {len(main_df.columns)} columns\")\n",
    "\n",
    "# Verify the column was removed (optional)\n",
    "print(\"\\nRemaining columns:\", main_df.columns.tolist())"
   ],
   "id": "95a286dc72545c65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current columns: ['Unnamed: 0', 'id', 'last_scraped', 'name', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month', 'bedrooms_imputed_method', 'beds_imputed_method', 'avg_price', 'price_period_start', 'price_period_end', 'number_of_listings', 'price_std', 'distance_to_closest_venue', 'distance_to_closest_station_km']\n",
      "Saved dataset with 107937 rows and 58 columns\n",
      "\n",
      "Remaining columns: ['Unnamed: 0', 'id', 'last_scraped', 'name', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month', 'bedrooms_imputed_method', 'beds_imputed_method', 'avg_price', 'price_period_start', 'price_period_end', 'number_of_listings', 'price_std']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9ad4c933b708e05"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
