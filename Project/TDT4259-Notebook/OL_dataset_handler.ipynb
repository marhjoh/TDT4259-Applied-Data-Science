{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Append lat/lon to Olympic venues",
   "id": "bcc9db68d040336c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T13:15:49.322504Z",
     "start_time": "2024-10-12T13:15:16.815729Z"
    }
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "def geocode_venue(venue_name, access_token):\n",
    "    base_url = \"https://api.mapbox.com/geocoding/v5/mapbox.places/\"\n",
    "    encoded_venue = urllib.parse.quote(venue_name)\n",
    "    url = f\"{base_url}{encoded_venue}.json\"\n",
    "    params = {\n",
    "        'access_token': access_token,\n",
    "        'country': 'FR',\n",
    "        'bbox': '2.224199,48.815573,2.469921,48.902145',\n",
    "        'limit': 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data['features']:\n",
    "        coordinates = data['features'][0]['center']\n",
    "        return coordinates[1], coordinates[0]  # returns lat, lon\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Usage\n",
    "access_token = 'YOUR_MAPBOX_ACCESS_TOKEN'\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv('paris_2024_data/venues.csv')\n",
    "\n",
    "# Add new columns for latitude and longitude\n",
    "df['latitude'] = None\n",
    "df['longitude'] = None\n",
    "\n",
    "# Iterate through venues and geocode\n",
    "for index, row in df.iterrows():\n",
    "    lat, lon = geocode_venue(row['venue'], access_token)\n",
    "    df.at[index, 'latitude'] = lat\n",
    "    df.at[index, 'longitude'] = lon\n",
    "\n",
    "# Save the enriched dataset\n",
    "df.to_csv('paris_2024_data/venues_with_coordinates.csv', index=False)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Append neighborhood",
   "id": "602ec92ee7f7adee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:37:55.133893Z",
     "start_time": "2024-10-12T13:37:55.093017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Read the CSV file with venue coordinates\n",
    "venues_df = pd.read_csv('paris_2024_data/venues_with_coordinates.csv')\n",
    "\n",
    "# Read the GeoJSON file\n",
    "with open('geojson/neighbourhoods.geojson', 'r') as f:\n",
    "    neighborhoods_data = json.load(f)\n",
    "\n",
    "# Create a list of neighborhood polygons\n",
    "neighborhoods = []\n",
    "for feature in neighborhoods_data['features']:\n",
    "    name = feature['properties']['neighbourhood']\n",
    "    # Check if the geometry is a Polygon or MultiPolygon\n",
    "    if feature['geometry']['type'] == 'Polygon':\n",
    "        poly = Polygon(feature['geometry']['coordinates'][0])\n",
    "        neighborhoods.append((name, poly))\n",
    "    elif feature['geometry']['type'] == 'MultiPolygon':\n",
    "        for polygon in feature['geometry']['coordinates']:\n",
    "            poly = Polygon(polygon[0])\n",
    "            neighborhoods.append((name, poly))\n",
    "\n",
    "# Function to find the neighborhood for a point\n",
    "def find_neighborhood(lat, lon):\n",
    "    point = Point(lon, lat)\n",
    "    for name, poly in neighborhoods:\n",
    "        if poly.contains(point):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "# Apply the function to each venue\n",
    "venues_df['neighbourhood'] = venues_df.apply(lambda row: find_neighborhood(row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "venues_df.to_csv('paris_2024_data/venues_with_neighborhoods.csv', index=False)\n",
    "\n",
    "print(\"Process completed. Check 'paris_2024_data/venues_with_neighborhoods.csv' for the result.\")"
   ],
   "id": "cf64daa489bebad9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed. Check 'paris_2024_data/venues_with_neighborhoods.csv' for the result.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjokk\\miniconda3\\Lib\\site-packages\\shapely\\predicates.py:526: RuntimeWarning: invalid value encountered in contains\n",
      "  return lib.contains(a, b, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fix neighborhood names",
   "id": "8b930d408433e2ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:46:19.130762Z",
     "start_time": "2024-10-12T13:46:19.115789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correct neighborhood names\n",
    "correct_names = [\n",
    "    \"Observatoire\", \"Hôtel-de-Ville\", \"Entrepôt\", \"Opéra\", \"Vaugirard\",\n",
    "    \"Louvre\", \"Luxembourg\", \"Popincourt\", \"Gobelins\", \"Bourse\",\n",
    "    \"Buttes-Montmartre\", \"Buttes-Chaumont\", \"Temple\", \"Reuilly\",\n",
    "    \"Élysée\", \"Panthéon\", \"Batignolles-Monceau\", \"Ménilmontant\",\n",
    "    \"Palais-Bourbon\", \"Passy\"\n",
    "]\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('paris_2024_data/venues_with_neighborhoods.csv')\n",
    "\n",
    "# Function to find the closest match\n",
    "def find_closest_match(name, correct_list):\n",
    "    return min(correct_list, key=lambda x: sum(c1 != c2 for c1, c2 in zip(name.lower(), x.lower())))\n",
    "\n",
    "# Correct the neighborhood names\n",
    "df['neighbourhood'] = df['neighbourhood'].apply(lambda x: find_closest_match(str(x), correct_names) if pd.notnull(x) else x)\n",
    "\n",
    "# Save the corrected CSV\n",
    "df.to_csv('paris_2024_data/venues_with_corrected_neighborhoods.csv', index=False)\n",
    "\n",
    "print(\"Neighborhood names have been corrected. Check 'venues_with_corrected_neighborhoods.csv' for the result.\")"
   ],
   "id": "1967d8517393024f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighborhood names have been corrected. Check 'venues_with_corrected_neighborhoods.csv' for the result.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neighborhood geojson to usable csv",
   "id": "17f496d085909ae2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3e9df3ae7e0ae36b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:15:34.548964Z",
     "start_time": "2024-10-12T15:15:34.380731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def geojson_to_csv(input_file, output_file):\n",
    "    # Read the GeoJSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        geojson_data = json.load(f)\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow(['neighbourhood', 'contour'])\n",
    "\n",
    "        # Process each feature in the GeoJSON\n",
    "        for feature in geojson_data['features']:\n",
    "            neighbourhood = feature['properties']['neighbourhood']\n",
    "            coordinates = feature['geometry']['coordinates']\n",
    "\n",
    "            # Extract and format coordinates\n",
    "            contour = []\n",
    "            for polygon in coordinates:\n",
    "                for ring in polygon:\n",
    "                    contour.extend(ring)\n",
    "\n",
    "            # Write the row to CSV\n",
    "            writer.writerow([neighbourhood, contour])\n",
    "\n",
    "    print(f\"CSV file '{output_file}' has been created successfully.\")\n",
    "\n",
    "# Usage\n",
    "input_file = 'geojson/neighbourhoods.geojson'\n",
    "output_file = 'geojson/neighbourhoods.csv'\n",
    "geojson_to_csv(input_file, output_file)"
   ],
   "id": "15fe6fda31ebafb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'geojson/neighbourhoods.csv' has been created successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Appending capacity to dataset",
   "id": "58c273601238c939"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T16:46:01.735715Z",
     "start_time": "2024-10-12T16:46:01.715434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_venue_capacity(existing_data_file, capacity_data, output_file):\n",
    "    # Read the existing dataset\n",
    "    existing_df = pd.read_csv(existing_data_file)\n",
    "\n",
    "    # Create a dictionary of venue capacities\n",
    "    capacity_dict = {\n",
    "        'Aquatics Centre': 5000,\n",
    "        'Bercy Arena': 15000,\n",
    "        'Champ de Mars Arena': 9000,\n",
    "        'Château de Versailles': 80000,\n",
    "        'Chateauroux Shooting Centre': 3000,\n",
    "        'Eiffel Tower Stadium': 12000,\n",
    "        'Elancourt Hill': 25000,\n",
    "        'Bordeaux Stadium': 42000,\n",
    "        'La Beaujoire Stadium': 35000,\n",
    "        'Geoffroy-Guichard Stadium': 42000,\n",
    "        'Parc des Princes': 50000,\n",
    "        'Lyon Stadium': 60000,\n",
    "        'Marseille Stadium': 68000,\n",
    "        'Nice Stadium': 35000,\n",
    "        'Grand Palais': 8000,\n",
    "        'Invalides': 8000,\n",
    "        'La Concorde': 30000,\n",
    "        'Le Bourget': 5000,\n",
    "        'Le Golf National': 35000,\n",
    "        'Marseille Marina': 5000,\n",
    "        'North Paris Arena': 6000,\n",
    "        'Paris La Defense Arena': 15000,\n",
    "        'Pierre Mauroy Stadium': 26000,\n",
    "        'Pont Alexandre III': 1000,\n",
    "        'Porte de La Chapelle Arena': 8000,\n",
    "        'Stade Roland-Garros': 36000,\n",
    "        'Saint-Quentin Velodrome': 5000,\n",
    "        'South Paris Arena': 6000,\n",
    "        'Stade de France': 77000,\n",
    "        'Tahiti': 5000,\n",
    "        'Trocadéro': 13000,\n",
    "        'Vaires-sur-Marne Nautical Stadium': 22000,\n",
    "        'Yves-du-Manoir Stadium': 15000\n",
    "    }\n",
    "\n",
    "    # Add capacity column to the existing dataframe\n",
    "    existing_df['capacity'] = existing_df['venue'].map(capacity_dict)\n",
    "\n",
    "    # Save the merged dataset to a new CSV file\n",
    "    existing_df.to_csv(output_file, index=False)\n",
    "    print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "existing_data_file = 'paris_2024_data/venues_with_corrected_neighborhoods.csv'\n",
    "output_file = 'paris_2024_data/venues_with_corrected_neighborhoods_and_capacity.csv'\n",
    "\n",
    "merge_venue_capacity(existing_data_file, None, output_file)"
   ],
   "id": "4c368a38c9274799",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to paris_2024_data/venues_with_corrected_neighborhoods_and_capacity.csv\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3a417a34fde1f69b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
